Think about using rollout n numbers vs probablibiteis - deals with nonzero tempereature better

change convolutional layer - test 4 or 5 size

debug model - show loss?

Will need to change cross entropy - see if there is a better alternative
Maybe run supervised learning on sample games to debug?
#Change model loading to be done via queues (maybe?) Also for saving memory, models
# Need to fix overfitting issues - might be related to above

#Deal with how it works with enemy moves better - read up!!!!!!!!
#ADD enemy one step lookahead


#add folder per run for saves
#fix overflow/power stuff
#write full stack trace error
#add seperate resume triggers for memory/model

ADD MCTS evaluator

#save buffer as well as network stategcloud au

check if it works in simple case
add multithreading capability
check on connect 4
add async self-play - network training
add skip connection
Save memory buffer as well as model weights





TODO (in rough order):
have output being the action space (done)
change it to memory buffers (done)]
stop illegal moves (done)
stop weights increasing too much (done I think)
add convolutional networks (done)

figure out why weights are increasing (tried clamping to fix?)
Model overfitting - think about dropout layer, prioritized experience replay

#TODO make sure copies are consistent

add evaluation against random player

Add learning rate
add target network (done)
add prioritized experience replay (done)
save models (done)
Think about evalution of models
add dropout layers (done)
add gpu(done)
optimize code for gpu
pararelize cpu code

maybe try softmax instead of epsilon greedy (for evaluation?)

#add higher order logic on self play (MCTS)




change from q learning to something else (td lambda? n step tree backup?)
convert everything to tensor before it hits networK???????????????
