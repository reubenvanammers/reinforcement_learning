Deal with how it works with enemy moves better - read up!!!!!!!!


ADD MCTS evaluator
check if it works in simple case
add multithreading capability
check on connect 4
add async self-play - network training
add skip connection
Save memory buffer as well as model weights





TODO (in rough order):
have output being the action space (done)
change it to memory buffers (done)]
stop illegal moves (done)
stop weights increasing too much (done I think)
add convolutional networks (done)

figure out why weights are increasing (tried clamping to fix?)
Model overfitting - think about dropout layer, prioritized experience replay

#TODO make sure copies are consistent

add evaluation against random player

Add learning rate
add target network (done)
add prioritized experience replay (done)
save models (done)
Think about evalution of models
add dropout layers (done)
add gpu(done)
optimize code for gpu
pararelize cpu code

maybe try softmax instead of epsilon greedy (for evaluation?)

#add higher order logic on self play (MCTS)




change from q learning to something else (td lambda? n step tree backup?)
convert everything to tensor before it hits networK???????????????
